{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9KHnTq-rav6",
        "outputId": "ddbaa454-bf01-4f52-a02c-c1846a81b8d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-whois\n",
            "  Downloading python_whois-0.9.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from python-whois) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->python-whois) (1.17.0)\n",
            "Downloading python_whois-0.9.5-py3-none-any.whl (104 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-whois\n",
            "Successfully installed python-whois-0.9.5\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2025.4.26)\n",
            "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-2.1.0 tldextract-5.3.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from multiprocessing import Pool, cpu_count\n",
        "!pip install python-whois\n",
        "import whois\n",
        "import re\n",
        "!pip install tldextract\n",
        "import tldextract\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "import ssl\n",
        "import socket\n",
        "from datetime import timezone\n",
        "import requests\n",
        "import time\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import idna\n",
        "import unicodedata\n",
        "\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABzacAmGzT-v"
      },
      "outputs": [],
      "source": [
        "columns=[\n",
        "#total url related parameters\n",
        "    'URL','URL_Length','Url_Shortening','Ssl_Info',\n",
        "    #\"\"\"'AsNo',\"\"\"\n",
        "    'Global_Ranking','Has_Suspicious_Params','Num_Languages',\n",
        "#Scheme\n",
        "    'Uses_HTTPS',\n",
        "#User Info\tuser\n",
        "    'Has_User_Info','User_Info_Length',\n",
        "#Subdomain\n",
        "    'Num_Subdomains',\"Num_Digits_Subdomain\",'Num_Hyphens_Subdomain','Length_Subdomain','Num_Dots_Subdomain','Num_Dash_Subdomain',\n",
        "    'Num_AtSymbol_Subdomain','Num_TildeSymbol_Subdomain','Num_Underscore_Subdomain','Num_Percent_Subdomain','Num_Ampersand_Subdomain', 'Num_Hash_Subdomain',\n",
        "    #\"\"\"'DomainInSubdomains',\"\"\"\n",
        "          'Hex_Encoded_Characters_Subdomain',\n",
        "#Domain\n",
        "    'Has_Ip',\"Num_Digits_Domain\",'Num_Hyphens_Domain','Length_Domain','Num_Dots_Domain','Num_Dash_Domain','Num_AtSymbol_Domain',\n",
        "    'Domain_Age','Num_TildeSymbol_Domain','Num_Underscore_Domain','Num_Percent_Domain','Num_Ampersand_Domain','Num_Hash_Domain', 'Hex_Encoded_Characters_Domain',\n",
        "#TLD\n",
        "    'TLD',\n",
        "#Port\n",
        "    'Has_Port',\n",
        "#Path\n",
        "    'Path_Level',\"Num_Digits_Path\",'Num_Hyphens_Path','Length_Path','Num_Dots_Path','Num_Dash_Path','Num_AtSymbol_Path',\n",
        "    'Num_TildeSymbol_Path','Num_Underscore_Path','Num_Percent_Path','Num_Ampersand_Path','Num_Hash_Path','Num_DoubleSlash_Path',\n",
        "#\"\"\"'DomainInPaths',\"\"\"\n",
        "    'Hex_Encoded_Characters_Path',\n",
        "#Query\n",
        "    \"Num_Digits_Query\",'Num_Hyphens_Query','Length_Query','Num_Query_Params','Num_Dots_Query','Num_Dash_Query','Num_AtSymbol_Query',\n",
        "    'Num_TildeSymbol_Query','Num_Underscore_Query','Num_Percent_Query','Num_Ampersand_Query','Num_Hash_Query','Hex_Encoded_Characters_Query',\n",
        "#Fragment\n",
        "    \"Has_Fragment\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgGYy_6e_uuZ"
      },
      "outputs": [],
      "source": [
        "suspicious_url_parameters = {\n",
        "    # Open Redirects & Phishing\n",
        "    \"redirect\", \"next\", \"url\", \"dest\", \"out\", \"view\", \"link\", \"goto\", \"target\", \"forward\", \"jump\",\n",
        "\n",
        "    # Remote Code Execution (RCE)\n",
        "    \"cmd\", \"eval\", \"execute\", \"exec\", \"shell\", \"run\", \"code\", \"command\", \"process\", \"do\",\n",
        "\n",
        "    # File Inclusion & Download Attacks (LFI & RFI)\n",
        "    \"file\", \"filepath\", \"download\", \"include\", \"load\", \"document\", \"pdf\", \"attachment\",\n",
        "    \"export\", \"import\", \"module\", \"dir\", \"read\", \"write\", \"stream\", \"content\", \"getfile\",\n",
        "\n",
        "    # Download-related extensions\n",
        "    \"exe\", \"msi\", \"apk\", \"bat\", \"sh\", \"cmd\", \"jar\", \"scr\", \"zip\", \"rar\", \"7z\", \"tar\", \"gz\",\n",
        "    \"pdf\", \"doc\", \"docx\", \"xls\", \"xlsx\", \"ppt\", \"pptx\", \"iso\", \"img\", \"vbs\", \"ps1\", \"dll\", \"reg\",\n",
        "\n",
        "    # SQL Injection (SQLi)\n",
        "    \"id\", \"query\", \"search\", \"order\", \"sort\", \"filter\", \"select\", \"drop\", \"union\", \"table\",\n",
        "    \"from\", \"where\", \"update\", \"delete\", \"insert\", \"fetch\", \"database\", \"schema\",\n",
        "\n",
        "    # Authentication Bypass & Session Hijacking\n",
        "    \"token\", \"session\", \"auth\", \"jwt\", \"sso\", \"apikey\", \"password\", \"pass\", \"user\", \"username\",\n",
        "    \"email\", \"key\", \"credential\", \"login\", \"logout\", \"csrf\", \"remember\", \"access\", \"validate\",\n",
        "\n",
        "    # XSS (Cross-Site Scripting)\n",
        "    \"script\", \"alert\", \"onload\", \"onerror\", \"onmouseover\", \"onclick\", \"onfocus\",\n",
        "    \"data\", \"src\", \"href\", \"javascript\", \"vbscript\", \"expression\", \"cookie\", \"input\",\n",
        "\n",
        "    # Command Injection\n",
        "    \"ping\", \"nc\", \"ncat\", \"curl\", \"wget\", \"bash\", \"sh\", \"zsh\", \"perl\", \"python\",\n",
        "\n",
        "    # Path Traversal & Directory Access\n",
        "    \"path\", \"dir\", \"folder\", \"root\", \"home\", \"var\", \"etc\", \"passwd\", \"config\", \"secret\",\n",
        "\n",
        "    # API & Debugging\n",
        "    \"debug\", \"test\", \"trace\", \"log\", \"verbose\", \"info\", \"error\", \"status\", \"diagnostic\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGSPibx77qSi"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_URL = \"https://tranco-list.eu/api\"\n",
        "\n",
        "def get_latest_tranco_rank(domain):\n",
        "    url = f\"{BASE_URL}/ranks/domain/{domain}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        ranks = response.json().get(\"ranks\", [])\n",
        "\n",
        "        if not ranks:\n",
        "            return 0\n",
        "\n",
        "        ranks.sort(key=lambda x: x[\"date\"], reverse=True)\n",
        "\n",
        "        return ranks[0][\"rank\"]\n",
        "\n",
        "    elif response.status_code == 403:\n",
        "        return -1\n",
        "    elif response.status_code == 429:\n",
        "        return -1\n",
        "    else:\n",
        "        return -1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd0IzSmuAil4"
      },
      "outputs": [],
      "source": [
        "def decode_idn(domain):\n",
        "    \"\"\"Decode multi-layer IDN domains (handles double Punycode encoding).\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            decoded = idna.decode(domain)\n",
        "            if decoded == domain:\n",
        "                break\n",
        "            domain = decoded\n",
        "        except idna.IDNAError:\n",
        "            break\n",
        "    return domain\n",
        "IGNORE_CHARACTERS = [\n",
        "    \".\", \"-\", \"_\", \"~\", \"/\", \"?\", \"#\", \"&\", \"=\", \"%\", \"@\", \":\", \";\", \",\", \"+\", \"*\", \"!\", \"$\", \"'\", '\"', \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"|\", \"\\\\\"\n",
        "]\n",
        "def get_num_languages(domain):\n",
        "    \"\"\"Decode domain, identify character scripts, and count unique ones.\"\"\"\n",
        "\n",
        "    decoded_domain = decode_idn(domain)\n",
        "\n",
        "    scripts = set()\n",
        "\n",
        "    for char in decoded_domain:\n",
        "        if char in IGNORE_CHARACTERS:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            script_name = unicodedata.name(char).split()[0]  # Get script category\n",
        "            scripts.add(script_name)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return len(scripts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLvgKxmn60ck"
      },
      "outputs": [],
      "source": [
        "def analyze_string_metrics(input_str):\n",
        "    \"\"\"Returns a dictionary with counts of various special characters in the input string.\"\"\"\n",
        "    return {\n",
        "        \"Num_Digits\": len(re.findall(r\"\\d\", input_str)),  # Count digits (0-9)\n",
        "        \"Num_Hyphens\": input_str.count(\"-\"),  # Count hyphens (-)\n",
        "        \"Length\": len(input_str),  # Total length of the string\n",
        "        \"Num_Dots\": input_str.count(\".\"),  # Count dots (.)\n",
        "        \"Num_Dash\": input_str.count(\"_\"),  # Count underscores (_)\n",
        "        \"Num_AtSymbol\": input_str.count(\"@\"),  # Count at symbols (@)\n",
        "        \"Num_TildeSymbol\": input_str.count(\"~\"),  # Count tilde (~)\n",
        "        \"Num_Underscore\": input_str.count(\"_\"),  # Count underscores (_)\n",
        "        \"Num_Percent\": input_str.count(\"%\"),  # Count percent symbols (%)\n",
        "        \"Num_Ampersand\": input_str.count(\"&\"),  # Count ampersands (&)\n",
        "        \"Num_Hash\": input_str.count(\"#\"),  # Count hash symbols (#)\n",
        "        \"Hex_Encoded_Characters\": len(re.findall(r\"%[0-9A-Fa-f]{2}\", input_str))  # Count hex-encoded characters\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UYVEaLdYd-V"
      },
      "outputs": [],
      "source": [
        "def process_Subdomain(subdomain):\n",
        "    num_subdomains = len([sub for sub in subdomain.split(\".\") if sub])\n",
        "    if subdomain.count('.')==0:\n",
        "        num_subdomains=0\n",
        "        fetures=analyze_string_metrics(\"\")\n",
        "    else:\n",
        "        subdomain= '.'.join(subdomain.split('.')[:-1])\n",
        "        fetures=analyze_string_metrics(subdomain)\n",
        "    fetures = {f\"{key}_Subdomain\": value for key, value in fetures.items()}\n",
        "    fetures['Num_Subdomains']=num_subdomains\n",
        "    return fetures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U7lN_m5bpxD"
      },
      "outputs": [],
      "source": [
        "def process_Domain(domain,suffix):\n",
        "    fetures=analyze_string_metrics(domain)\n",
        "    fetures = {f\"{key}_Domain\": value for key, value in fetures.items()}\n",
        "    fetures['Domain_Age'] = get_domain_age(domain+'.'+suffix)\n",
        "    fetures['Has_Ip'] = bool(re.match(r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", domain))\n",
        "    return fetures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r4ox0C34lqA"
      },
      "outputs": [],
      "source": [
        "def process_Path(path):\n",
        "    fetures={}\n",
        "    fetures=analyze_string_metrics(path)\n",
        "    fetures = {f\"{key}_Path\": value for key, value in fetures.items()}\n",
        "    if path=='':\n",
        "        fetures['Path_Level']=0\n",
        "    elif path=='/':\n",
        "        fetures['Path_Level']=1\n",
        "    else:\n",
        "        fetures['Path_Level']=path.count('/')\n",
        "    fetures['Num_DoubleSlash_Path']=path.count('//')\n",
        "    return fetures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtwO-0QRwUyx"
      },
      "outputs": [],
      "source": [
        "def process_Query(query):\n",
        "    fetures={}\n",
        "    fetures=analyze_string_metrics(query)\n",
        "    fetures = {f\"{key}_Query\": value for key, value in fetures.items()}\n",
        "    fetures['Num_Query_Params'] = len(parse_qs(query))\n",
        "    return fetures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V_Zth2tx48w"
      },
      "outputs": [],
      "source": [
        "def get_domain_age(domain):\n",
        "    for _ in range(3):\n",
        "        try:\n",
        "            domain_info = whois.whois(domain)\n",
        "            creation_date = domain_info.creation_date\n",
        "\n",
        "            if isinstance(creation_date, list):\n",
        "                creation_date = [\n",
        "                    d.replace(tzinfo=timezone.utc) if d.tzinfo is None else d.astimezone(timezone.utc)\n",
        "                    for d in creation_date if isinstance(d, datetime)\n",
        "                ]\n",
        "                if not creation_date:\n",
        "                    continue\n",
        "                creation_date = min(creation_date)\n",
        "\n",
        "\n",
        "            if isinstance(creation_date, datetime):\n",
        "                if creation_date.tzinfo is None:\n",
        "                    creation_date = creation_date.replace(tzinfo=timezone.utc)\n",
        "                else:\n",
        "                    creation_date = creation_date.astimezone(timezone.utc)\n",
        "\n",
        "                return (datetime.now(timezone.utc) - creation_date).days\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Lookup failed for {domain}: {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ4GGkIgyBt5"
      },
      "outputs": [],
      "source": [
        "def get_user_info(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    username = parsed_url.username or \"\"\n",
        "    password = parsed_url.password or \"\"\n",
        "\n",
        "    has_user_info = bool(username or password)\n",
        "    user_info_length = len(username) + len(password)\n",
        "\n",
        "    return has_user_info, user_info_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs_UsYclyDxL"
      },
      "outputs": [],
      "source": [
        "def has_fragment(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return bool(parsed_url.fragment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAp1q9jdyI2n"
      },
      "outputs": [],
      "source": [
        "shorteners = eval(open('/content/url_shortener_extensions.txt','r').read())\n",
        "\n",
        "def is_shortened_url(url):\n",
        "    from urllib.parse import urlparse\n",
        "    domain = urlparse(url).netloc.lower()\n",
        "    return domain in shorteners"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G_dcLbjyMBu"
      },
      "outputs": [],
      "source": [
        "def has_port(url):\n",
        "    \"\"\"Returns True if the URL has an explicit port, otherwise False.\"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    return parsed_url.port is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeDJf1RmyPM2"
      },
      "outputs": [],
      "source": [
        "def get_ssl_certificate_info(hostname, port=443):\n",
        "    # 1 - valid, 0 - invalid, -1 - error\n",
        "    try:\n",
        "        context = ssl.create_default_context()\n",
        "        with socket.create_connection((hostname, port), timeout=5) as sock:\n",
        "            with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
        "                cert = ssock.getpeercert()\n",
        "\n",
        "                if not cert:\n",
        "                    return 0\n",
        "\n",
        "                # Extract issuer and subject\n",
        "                issuer = dict(x[0] for x in cert.get(\"issuer\", []))\n",
        "                subject = dict(x[0] for x in cert.get(\"subject\", []))\n",
        "\n",
        "                # Extract expiry date and convert it to timezone-aware\n",
        "                expiry_date = datetime.strptime(cert[\"notAfter\"], \"%b %d %H:%M:%S %Y %Z\")\n",
        "                expiry_date = expiry_date.replace(tzinfo=timezone.utc)  # Convert to offset-aware datetime\n",
        "\n",
        "                is_expired = expiry_date < datetime.now(timezone.utc)\n",
        "\n",
        "                # Check if self-signed\n",
        "                is_self_signed = issuer == subject\n",
        "\n",
        "                # Extract SAN (Subject Alternative Name)\n",
        "                san_list = [ext[1] for ext in cert.get(\"subjectAltName\", []) if ext[0] == \"DNS\"]\n",
        "\n",
        "                # Validate hostname against CN and SANs\n",
        "                valid_hostname = hostname in san_list or hostname == subject.get(\"commonName\", \"Unknown\")\n",
        "\n",
        "                # Issuer should be checked against a valid list of certificate authorities\n",
        "                if valid_hostname and (not is_expired and not is_self_signed):\n",
        "                    return 1\n",
        "                return 0\n",
        "    except ssl.SSLCertVerificationError:\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        print(f\"SSL check error for {hostname}: {e}\")\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7TH6pIVyP6i"
      },
      "outputs": [],
      "source": [
        "def has_fragment(url):\n",
        "    \"\"\"Check if a URL contains a fragment (#)\"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    return bool(parsed_url.fragment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLBCFGbd_2Uv"
      },
      "outputs": [],
      "source": [
        "\n",
        "multiprocessing.set_start_method(\"spawn\", force=True)\n",
        "\n",
        "def process_url(url):\n",
        "    try:\n",
        "        fetures = {}\n",
        "        fetures['URL'] = url\n",
        "        fetures['URL_Length'] = len(url)\n",
        "        fetures['Url_Shortening'] = is_shortened_url(url)\n",
        "\n",
        "        parsed = urlparse(url)\n",
        "        domain_info = tldextract.extract(url)\n",
        "        domain = f\"{domain_info.domain}.{domain_info.suffix}\"\n",
        "\n",
        "        fetures['Ssl_Info'] = None#get_ssl_certificate_info(domain)\n",
        "        fetures['Global_Ranking'] = get_latest_tranco_rank(domain)\n",
        "\n",
        "        query_params = parsed.query.split(\"&\") if parsed.query else []\n",
        "        fetures['Has_Suspicious_Params'] = any(param.split(\"=\")[0] in suspicious_url_parameters for param in query_params)\n",
        "\n",
        "        fetures['Num_Languages'] = get_num_languages(domain)\n",
        "        fetures['Uses_HTTPS'] = 1 if url.startswith(\"https\") else 0\n",
        "        fetures['Has_User_Info'], fetures['User_Info_Length'] = get_user_info(url)\n",
        "\n",
        "        subdomain_info = process_Subdomain(domain_info.subdomain)\n",
        "        fetures.update(subdomain_info)\n",
        "\n",
        "        Domain_info = process_Domain(domain_info.domain, domain_info.suffix)\n",
        "        fetures.update(Domain_info)\n",
        "\n",
        "        fetures['TLD'] = domain_info.suffix\n",
        "        fetures['Has_Port'] = has_port(url)\n",
        "\n",
        "        path_info = process_Path(parsed.path)\n",
        "        fetures.update(path_info)\n",
        "\n",
        "        query_info = process_Query(parsed.query)\n",
        "        fetures.update(query_info)\n",
        "\n",
        "        fetures['Has_Fragment'] = has_fragment(url)\n",
        "\n",
        "        return fetures\n",
        "\n",
        "    except Exception as e:\n",
        "        # Optionally log the error\n",
        "        # print(f\"Error processing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_url_features_parallel(urls, num_workers=None):\n",
        "    num_workers = min(num_workers or cpu_count(), 8)  # Optimize for I/O tasks\n",
        "\n",
        "    with ThreadPool(num_workers) as pool:  # Thread-based pool for speedup\n",
        "        data = list(tqdm(pool.imap(process_url, urls), total=len(urls), desc=\"Processing URLs\"))\n",
        "\n",
        "    # Remove None values (failed URLs)\n",
        "    data = [entry for entry in data if entry]\n",
        "\n",
        "    # Ensure each row has all expected columns\n",
        "    features = [{col: entry.get(col, None) if isinstance(entry, dict) else None for col in columns} for entry in data]\n",
        "\n",
        "    return pd.DataFrame(features, columns=columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcrjTepEgjce"
      },
      "outputs": [],
      "source": [
        "# Load your phishtank database\n",
        "phistankDB = pd.read_csv('/content/phistank.csv')\n",
        "\n",
        "phistankDB = phistankDB.sample(n=22000, random_state=42)['url']\n",
        "phistankDB.to_csv('22000_pt.csv', index=False)\n",
        "# Get the URL list (from the column index 1)\n",
        "#url_list = phistankDB.iloc[:, 1].tolist()\n",
        "#print(len(url_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr6UC56iBqwz"
      },
      "outputs": [],
      "source": [
        "t1m = pd.read_csv('/content/top-1m.csv')\n",
        "t1m = t1m.sample(n=22000, random_state=42).iloc[:,[1]]\n",
        "t1m.columns=['url']\n",
        "t1m.to_csv('22000_1m.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRVhAoKZDzEs"
      },
      "outputs": [],
      "source": [
        "phistankDB = pd.read_csv('/content/22000_1m.csv')\n",
        "url_list = phistankDB.iloc[:, 0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0AN3S_dgu1f",
        "outputId": "60e6459e-239c-4245-8c8d-f7951ff63cee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing URLs: 100%|██████████| 12000/12000 [7:37:26<00:00,  2.29s/it]\n"
          ]
        }
      ],
      "source": [
        "# Process a chunk (e.g., from index 4000 to 6000)\n",
        "df = extract_url_features_parallel(url_list[10000:22000])\n",
        "df.to_csv('top1m(10000_22000).csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X542xUQIHzwP"
      },
      "outputs": [],
      "source": [
        "phistankDB = pd.read_csv('/content/phistank.csv')\n",
        "phistankDB = phistankDB.sample(n=10000, random_state=42)\n",
        "url_list = phistankDB.iloc[:,1].tolist()\n",
        "df = extract_url_features_parallel(url_list[4000:6000])\n",
        "df.to_csv('top1m(4000:6000).csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YoQURhd35Tz",
        "outputId": "84eaedab-c062-4442-b51b-ccaa8f97c8d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing URLs: 100%|██████████| 200/200 [04:41<00:00,  1.41s/it]\n",
            "Processing URLs: 100%|██████████| 200/200 [07:15<00:00,  2.18s/it]\n"
          ]
        }
      ],
      "source": [
        "#valisarion set\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "phistankDB = pd.read_csv('/content/phistank.csv')\n",
        "t1m = pd.read_csv('/content/top-1m.csv')\n",
        "\n",
        "url_list_pt = phistankDB.iloc[:, 1].tolist()\n",
        "url_list_1m = t1m.iloc[:, 1].tolist()\n",
        "\n",
        "already_used = pd.concat([pd.read_csv('/content/combined_phishtank.csv'), pd.read_csv('/content/combined_top1m.csv')], ignore_index=True)\n",
        "already_used_urls = set(already_used['URL'])  # assuming column is named 'URL'\n",
        "\n",
        "filtered_urls_pt = [url for url in url_list_pt if url not in already_used_urls]\n",
        "filtered_urls_1m = [url for url in url_list_1m if url not in already_used_urls]\n",
        "\n",
        "# Sample 200 from each\n",
        "selected_urls_pt = random.sample(filtered_urls_pt, 200)\n",
        "selected_urls_1m = random.sample(filtered_urls_1m, 200)\n",
        "\n",
        "# Feature extraction\n",
        "df_pt = extract_url_features_parallel(selected_urls_pt)\n",
        "df_1m = extract_url_features_parallel(selected_urls_1m)\n",
        "\n",
        "# Add labels\n",
        "df_pt['label'] = 1\n",
        "df_1m['label'] = 0\n",
        "\n",
        "# Combine and save\n",
        "df = pd.concat([df_pt, df_1m], ignore_index=True)\n",
        "df.to_csv('validation_ds.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwxIIcSzxXx8"
      },
      "outputs": [],
      "source": [
        "#expanding the dataset\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "start=5000\n",
        "end=10000\n",
        "\n",
        "\n",
        "phistankDB = pd.read_csv('/content/phistank.csv')\n",
        "t1m = pd.read_csv('/content/top-1m.csv')\n",
        "\n",
        "url_list_pt = phistankDB.iloc[:, 1].tolist()\n",
        "url_list_1m = t1m.iloc[:, 1].tolist()\n",
        "\n",
        "already_used = pd.concat([pd.read_csv('/content/combined_phishtank.csv'), pd.read_csv('/content/combined_top1m.csv')], ignore_index=True)\n",
        "already_used_urls = set(already_used['URL'])  # assuming column is named 'URL'\n",
        "\n",
        "filtered_urls_pt = [url for url in url_list_pt if url not in already_used_urls]\n",
        "filtered_urls_1m = [url for url in url_list_1m if url not in already_used_urls]\n",
        "\n",
        "\n",
        "#selected_urls_pt = random.sample(filtered_urls_pt, 200)\n",
        "#selected_urls_1m = random.sample(filtered_urls_1m, 200)\n",
        "random.seed(42)\n",
        "random.shuffle(filtered_urls_pt)\n",
        "random.seed(42)\n",
        "random.shuffle(filtered_urls_1m)\n",
        "\n",
        "\n",
        "# Feature extraction\n",
        "df_pt = extract_url_features_parallel(filtered_urls_pt[start:end])\n",
        "df_1m = extract_url_features_parallel(filtered_urls_1m[start:end])\n",
        "\n",
        "# Add labels\n",
        "df_pt['label'] = 1\n",
        "df_1m['label'] = 0\n",
        "\n",
        "# File paths in Google Drive (update path if needed)\n",
        "output_folder = '/content/drive/MyDrive/URL_Feature_Sets/'\n",
        "file_pt = f'{output_folder}phistank({start+22000}_{end+22000}).csv'\n",
        "file_1m = f'{output_folder}top1m({start+22000}_{end+22000}).csv'\n",
        "\n",
        "# Save directly to Drive\n",
        "df_pt.to_csv(file_pt, index=False)\n",
        "df_1m.to_csv(file_1m, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
